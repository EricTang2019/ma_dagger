{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade azureml-core\n",
    "# pip install --upgrade azureml-sdk[notebooks,contrib]\n",
    "# pip install azure-ai-ml\n",
    "# pip install azure-identity\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core import ScriptRunConfig, Environment, Experiment, Workspace, Dataset, Model\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import pandas as pd\n",
    "from azure.ai.ml import command, PyTorchDistribution\n",
    "from azure.ai.ml.entities import JupyterLabJobService, SshJobService, TensorBoardJobService, VsCodeJobService\n",
    "from azure.ai.ml.entities._assets.environment import Environment\n",
    "from azure.identity import DefaultAzureCredential, ManagedIdentityCredential, AzureCliCredential\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/t-ziyanwang/.bashrc', 'r') as f:\n",
    "    bashrc = f.read() \n",
    "# Get WANDB_TOKEN and HF_TOKEN from bashrc\n",
    "wandb_token = re.search(r'export WANDB_TOKEN=(.*)', bashrc)\n",
    "hf_token = re.search(r'export HF_TOKEN=(.*)', bashrc)\n",
    "if wandb_token:\n",
    "    wandb_token = wandb_token.group(1).strip().strip('\"')\n",
    "if hf_token:\n",
    "    hf_token = hf_token.group(1).strip().strip('\"')\n",
    "# print(f'WANDB_TOKEN: {wandb_token}')\n",
    "# print(f'HF_TOKEN: {hf_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WANDB_TOKEN:\", wandb_token)\n",
    "print(\"HF_TOKEN:\", hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import posixpath\n",
    "os.chdir('/home/t-ziyanwang/intern/ma_dagger')\n",
    "print('CWD set to', os.getcwd())\n",
    "\n",
    "online_env_name = \"vllm-openai-0-9-1-custom\"\n",
    "\n",
    "candidate_vcs = [\n",
    "    # (\"msrresrchvc\", \"Singularity.NC96ad_A100_v4\", \"Premium\", \"High\", 4),\n",
    "    # (\"msrresrchvc\", \"Singularity.ND48_v4\", \"Premium\", \"High\", 4),\n",
    "    (\"msrresrchvc\", \"Singularity.ND96amrs_A100_v4\", \"Basic\", \"High\", 8),\n",
    "    (\"msrresrchbasicvc\", \"Singularity.NC96ad_A100_v4\", \"Standard\", \"High\", 8),\n",
    "    (\"msrresrchbasicvc\", \"Singularity.ND96amrs_A100_v4\", \"Basic\", \"High\", 8),\n",
    "    (\"msrresrchbasicvc\", \"Singularity.ND96_v4\", \"Basic\", \"High\", 8),\n",
    "    (\"msrresrchbasicvc\", \"Singularity.ND96_H100_v5\", \"Basic\", \"High\", 8),\n",
    "    (\"msrresrchvc\", \"Singularity.ND48_H100_v5\", \"Premium\", \"High\", 8),\n",
    "]\n",
    "\n",
    "candidate_models = [\n",
    "    (\"qwen3-4b\", \"Qwen/Qwen3-4B\"),\n",
    "]\n",
    "\n",
    "candidate_datasets = [\n",
    "    (\"aimo\", \"AI-MO/NuminaMath-CoT\", 8),\n",
    "]\n",
    "\n",
    "output_base = f'azureml://subscriptions/d4fe558f-6660-4fe7-99ec-ae4716b5e03f/resourcegroups/aifrontiers/workspaces/aifrontiers_ws/datastores/ziyanwang_data/paths/'\n",
    "num_shards = len(candidate_vcs)\n",
    "\n",
    "for shard_id, (vc, instance_type, sla_tier, priority, gpu_num) in enumerate(candidate_vcs):\n",
    "    for model_short_name, model_full_name in candidate_models:\n",
    "        for dataset_short_name, dataset_full_name, n in candidate_datasets:\n",
    "            run_base = f\"collect_data_{model_short_name}_{dataset_short_name}\"\n",
    "            vc_tag = f\"{vc}-{instance_type}\".replace('.', '-')\n",
    "            shard_tag = f\"shard{shard_id:02d}\"\n",
    "            runid = f\"p1-madagger-{dataset_short_name}-n{n}-{model_short_name}-{vc_tag}-{shard_tag}\"\n",
    "            experiment_name = 'ma_dagger'\n",
    "            remote_subdir = posixpath.join(run_base, f\"{vc_tag}_{shard_tag}\")\n",
    "            job_command_list = [\n",
    "                'export WANDB_API_KEY=5f642e1080557e1b07a844b75f8f580e7ff47791 && export WANDB_TOKEN=5f642e1080557e1b07a844b75f8f580e7ff47791 && export WANDB_ENTITY=kcl_coopai && export WANDB_PROJECT=madagger &&',\n",
    "                'huggingface-cli login --token ${{inputs.hf_token}} &&',\n",
    "                'wandb login ${WANDB_API_KEY} --host https://api.wandb.ai &&',\n",
    "                'free -h &&',\n",
    "                'pip3 install -r requirements.txt && pip3 install -e ./rllm && pip3 install -e ./collect_traces/openai-python &&',\n",
    "                'export GPU_COUNT=$(nvidia-smi -L | wc -l) && '\n",
    "                'if [ \"$GPU_COUNT\" -ge 8 ]; then GEN_CUDA=\"0,1,2,3\"; VER_CUDA=\"4,5,6,7\"; TP_PER_STUDENT=4; '\n",
    "                'elif [ \"$GPU_COUNT\" -ge 4 ]; then GEN_CUDA=\"0,1\"; VER_CUDA=\"2,3\"; TP_PER_STUDENT=2; '\n",
    "                'else GEN_CUDA=\"0\"; VER_CUDA=\"1\"; TP_PER_STUDENT=1; fi &&',\n",
    "                'echo \"gen gpus=$GEN_CUDA ver gpus=$VER_CUDA tp=$TP_PER_STUDENT\" &&',\n",
    "                f\"mkdir -p ${{outputs.output_dir}} && VLLM_WORKER_MULTIPROC_METHOD=spawn WANDB_API_KEY=5f642e1080557e1b07a844b75f8f580e7ff47791 WANDB_ENTITY=kcl_coopai WANDB_PROJECT=madagger \"\n",
    "                f\"python3 gen_ver_dagger_fullft_vllm.py iterate \"\n",
    "                f\"--dataset_name {dataset_short_name} --split train --batch_tasks {n} \"\n",
    "                f\"--teacher_backend triapi --teacher_triapi_instance gcr/shared \"\n",
    "                f\"--teacher_triapi_deployment gpt-5.1-chat_2025-11-13 \"\n",
    "                f\"--teacher_triapi_scope api://trapi/.default --teacher_triapi_api_version 2024-12-01-preview \"\n",
    "                f\"--teacher_triapi_max_parallel 64 --teacher_triapi_timeout 300 \"\n",
    "                f\"--gen_base {model_full_name} --ver_base {model_full_name} \"\n",
    "                f\"--gen_tokenizer {model_full_name} --ver_tokenizer {model_full_name} \"\n",
    "                f\"--tp_s $TP_PER_STUDENT --tp_t 1 --gen_cuda $GEN_CUDA --ver_cuda $VER_CUDA \"\n",
    "                f\"--parallel 64 --out_dir ${{outputs.output_dir}} --project_name madagger --experiment_name {runid}\"\n",
    "            ]\n",
    "            inputs = {\n",
    "                'wandb_token': wandb_token,\n",
    "                'hf_token': hf_token,\n",
    "            }\n",
    "            output_dir_path = f\"{output_base.rstrip('/')}/{remote_subdir}\"\n",
    "            outputs = {\n",
    "                'output_dir': Output(type=AssetTypes.URI_FOLDER, path=output_dir_path, mode=InputOutputModes.RW_MOUNT)\n",
    "            }\n",
    "\n",
    "            node_count = 1\n",
    "            process_count_per_node = 1\n",
    "            job_name = runid\n",
    "\n",
    "            subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"d4fe558f-6660-4fe7-99ec-ae4716b5e03f\")\n",
    "            resource_group = os.getenv(\"RESOURCEGROUP_NAME\", default=\"aifrontiers\")\n",
    "            workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"aifrontiers_ws\")\n",
    "\n",
    "            class vc_info:\n",
    "                def __init__(self, subscription_id= \"156138e5-a3b1-48af-9e2e-883f4df3f457\", resource_group=\"gcr-singularity-lab\", vc=\"dell1\"):\n",
    "                    self.subscription_id = subscription_id\n",
    "                    self.resource_group = resource_group\n",
    "                    self.vc = vc\n",
    "                    self.compute_config= \"/subscriptions/\"+ subscription_id +\"/resourceGroups/\"+ resource_group +\"/providers/Microsoft.MachineLearningServices/virtualclusters/\" + vc\n",
    "            if vc in [\"dell1\", \"kings01\", \"kings02\", \"kings03\", \"kings04\", \"kings05\", \"kings06\", \"kings07\", \"kings08\", \"kings09\", \"kings10\", \"kings11\", \"kings12\", \"mckinley01\", \"mckinley02\", \"mckinley03\", \"mckinley04\", \"mckinley05\", \"mckinley06\", \"mckinley07\", \"mckinley08\", \"barlow01\", \"barlow02\", \"barlow03\", \"barlow04\", \"barlow05\", \"barlow06\", \"barlow07\", \"barlow08\", \"barlow09\", \"msrresrchlab\"]:\n",
    "                vc_info= vc_info(subscription_id= \"156138e5-a3b1-48af-9e2e-883f4df3f457\", resource_group=\"gcr-singularity-lab\", vc=vc)\n",
    "            elif vc in [\"baltic01\", \"baltic02\", \"baltic03\", \"baltic04\", \"baltic05\", \"baltic06\", \"baltic07\", \"baltic08\", \"baltic09\", \"baltic10\", \"baltic11\", \"baltic12\", \"huashanvc1\", \"huashanvc2\", \"huashanvc3\", \"huashanvc4\"]:\n",
    "                vc_info= vc_info(subscription_id= \"22da88f6-1210-4de2-a5a3-da4c7c2a1213\", resource_group=\"gcr-singularity\", vc=vc)\n",
    "            elif vc in [\"msrresrchvc\"]:\n",
    "                vc_info= vc_info(subscription_id= \"22da88f6-1210-4de2-a5a3-da4c7c2a1213\", resource_group=\"gcr-singularity-resrch\", vc=vc)\n",
    "            elif vc in [\"msroctovc\", 'whitney00', 'whitney08', 'palisades04', 'whitney14']:\n",
    "                vc_info= vc_info(subscription_id= \"d4404794-ab5b-48de-b7c7-ec1fefb0a04e\", resource_group=\"gcr-singularity-octo\", vc=vc)\n",
    "            elif vc in ['msrresrchbasicvc']:\n",
    "                vc_info = vc_info(subscription_id= \"22da88f6-1210-4de2-a5a3-da4c7c2a1213\", resource_group=\"gcr-singularity\", vc=vc)\n",
    "            elif vc in ['msroctobasicvc']:\n",
    "                vc_info = vc_info(subscription_id= \"d4404794-ab5b-48de-b7c7-ec1fefb0a04e\", resource_group=\"gcr-singularity-octo\", vc=vc)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown virtual cluster {vc}, please check the list of available VCs in the documentation.\")\n",
    "\n",
    "            ml_client = MLClient(\n",
    "                AzureCliCredential(), subscription_id, resource_group, workspace_name\n",
    "            )\n",
    "            print(ml_client)\n",
    "            print(vc)\n",
    "            print(outputs)\n",
    "\n",
    "            vc_config = {\n",
    "                \"instance_type\": instance_type,\n",
    "                \"instance_count\": node_count,\n",
    "                \"properties\" : {\n",
    "                    \"AISuperComputer\" : {\n",
    "                        \"interactive\" : False,\n",
    "                        \"slaTier\": sla_tier,\n",
    "                        \"priority\": priority,\n",
    "                        \"tensorboardLogDirectory\": \"/scratch/tensorboard_logs\",\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            env = ml_client.environments.get(name=online_env_name, version=\"1\")\n",
    "\n",
    "            job = command(\n",
    "                code='.',\n",
    "                command=' '.join(job_command_list),\n",
    "                inputs=inputs,\n",
    "                outputs=outputs,\n",
    "                environment=env,\n",
    "                environment_variables={\n",
    "                    'JOB_EXECUTION_MODE': \"basic\",\n",
    "                    'AZUREML_COMPUTE_USE_COMMON_RUNTIME': 'true',\n",
    "                    '_AZUREML_SINGULARITY_JOB_UAI': '/subscriptions/d4fe558f-6660-4fe7-99ec-ae4716b5e03f/resourcegroups/aifrontiers/providers/Microsoft.ManagedIdentity/userAssignedIdentities/aifrontiers',\n",
    "                },\n",
    "                compute=vc_info.compute_config,\n",
    "                resources=vc_config,\n",
    "                instance_count=node_count,\n",
    "                display_name=job_name,\n",
    "                experiment_name=experiment_name,\n",
    "                distribution= {\n",
    "                    \"type\": \"PyTorch\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(job.command)\n",
    "\n",
    "            returned_job = ml_client.jobs.create_or_update(job)\n",
    "            print(f\"Job URL: {returned_job.studio_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge shard outputs into a single dataset after jobs finish\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "model_short_name = \"qwen3-8b\"\n",
    "dataset_short_name = \"aimo\"\n",
    "run_base_dir = Path(\"collect_traces/collect_traces/runs\") / f\"collect_data_{model_short_name}_{dataset_short_name}\"\n",
    "\n",
    "if not run_base_dir.exists():\n",
    "    raise FileNotFoundError(f\"{run_base_dir} not found. Download job outputs locally before merging.\")\n",
    "\n",
    "merged_output_root = Path(\"collect_traces/datasets\")\n",
    "merged_output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "merge_cmd = [\"python\", \"collect_traces/build_dataset.py\",\n",
    "             \"--gen_data_dir\", str(run_base_dir),\n",
    "             \"--output_path\", str(merged_output_root),\n",
    "             \"--keep\", \"firstfull\",\n",
    "             \"--recursive\"]\n",
    "print(\"Running:\", ' '.join(merge_cmd))\n",
    "subprocess.run(merge_cmd, check=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
